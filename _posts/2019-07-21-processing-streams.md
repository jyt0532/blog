---
layout: post
title: Designing Data-Intensive Application - Stream Processing - 流處理 
comments: True 
subtitle: 流處理 - 流處理
tags: systemDesign 
author: jyt0532
excerpt: 本篇文章介紹流處理第三部分 - 流處理
---

這是Designing Data-Intensive Application的第三部分第二章節Part3: 流處理

流處理Part1 - 傳遞事件流

流處理Part2 - 數據庫與流

流處理Part3 - 流處理

本篇是系列文的Part3

本文所有圖片或代碼來自於原書內容
{% include copyright.html %}

## 流處理

我們在前兩篇系列文中 討論了流的來源(用戶活動事件, 寫入數據庫等等) 和流如何傳輸(直接通過消息傳送,通過消息代理,通過事件日誌等等)

最後就是你可以用流做什麼 以及如何處理它 一般來說有三個選項

1.你可以將事件中的數據寫入數據庫/緩存/搜索索引或類似的衍生存儲系統 然後被其他客戶端查詢

2.你能以某種方式將事件推送給用戶 比如發送郵件或推送通知

3.你可以處理一個或多個輸入流 並產生一個或多個輸出流

本篇文章會著重在第三項 **處理流以產生其他衍生流**



### 流處理的應用

長期以來 流處理一直用於監控目的 如果某個事件發生 單位希望能得到警報 比如說

1.欺詐檢測系統需要確定信用卡的使用模式是否有意外地變化 如果系統判斷此卡被盜刷 要馬上鎖卡

2.交易系統需要檢查金融市場的價格變化 並依指定的規則進行交易

3.製造系統需要監控工廠中機器的狀態 如果有故障要馬上找出問題

4.軍事和情報系統需要跟蹤潛在侵略者的活動 如果偵測到襲擊要預先應對

隨著時代進步 軟體業界也有越來越多關於流的應用

#### 應用1: 復合事件處理 Complex event processing

適用於需要搜索某些事件模式的應用 就像regular expression允許你在字符串中搜索特定字符模式的方式 CEP允許你指定規則以**在流中搜索某些事件模式**

CEP系統通常使用高層次的聲明式查詢語言(比如SQL)來描述應該檢測到的事件模式 通常也會在內部維持一個state machine(狀態機) 當匹配發現時 引擎發出一個復合事件(complex event) 並附有檢測到的事件模式細節

你也可以從另一個角度理解 復合事件處理和一般數據庫的使用方式正好相反 數據庫是將存儲資料視為最重要目的 查詢只是臨時的 過了沒找到就沒了 但復合事件處理則是把查詢當作最重要目的 流過了不符合就不管了

#### 應用2: 流分析

使用流處理的另一個領域是對流進行分析 通常是關注大量事件上的指標 比如

1.測量某種類型事件發生的頻率

2.滾動計算一段時間窗口內某個值的平均值

3.將當前的統計值與先前的時間區間的值對比(檢測趨勢 當指標比上週高或低時通知)

這些統計值通常是在固定時間區間內進行計算的 比如說過去五分鐘的QPS跟P99 

目前已經有許多分佈式流處理框架的設計都是針對**分析**設計的 比如Apache Storm, Spark Streaming, Flink, Concord, Samza, Kafka Streams, Google Cloud Dataflow, Azure Stream Analytics

#### 應用3: 維護物化視圖 Maintain materialized views

我們知道數據庫的[Change Capture](/2019/07/21/databases-and-streams/#變更數據捕獲-change-data-capture)可以用於維護衍生數據系統(緩存, 搜索索引和數據倉庫) 使其與來源數據庫保持同步 我們可以把這些視為物化視圖(materialized views): 在某個數據集上衍生出一個替代視圖以便高效查詢，並在底層數據變更時更新視圖

在[事件溯源](/2019/07/21/databases-and-streams/#事件溯源-event-sourcing)中 應用程序的狀態是透過apply事件日誌來維護 通常僅考慮某個時間窗口內的事件通常是不夠的 構建物化視圖可能需要任意時間段內的所有事件(除了那些可能由日誌壓縮丟棄的過時事件) 

實際上 你需要一個可以一直延伸到時間開端的窗口  Samza和Kafka Streams支持這種用法

#### 應用4: 在流上搜索

我們可以搜尋某些單獨的事件

比如說 當市場出現了符合某消費者要求的房地產 房地產網站可以通知客戶

傳統的搜尋引擎 是首先索引文件 然後在索引上跑查詢 但搜索流卻反了過來 查詢被存了下來 逐一搜尋流的每個事件

Elasticsearch的這種過濾器功能 是實現這種流搜索的一種選擇

### 時間推理

看完以上的應用 你會發現我們在處理流的時候 幾乎都需要跟時間打交道 尤其是用於分析目的時候

不像批處理 對於批處理的運算通常是確定性的 也就是不論你什麼時間運算 因為事件本身已經有個時間戳 所以運算完結果會一樣

但對流處理來說 許多框架使用處理機器上的本地系統時鐘來確定窗口 概念簡單 就假設事情發生跟事情處理的時間一樣 但要是真的有顯著的延遲 那很多處理都會不準確

#### 事件時間與處理時間

很多原因會導致處理延遲 比如說排隊 網路故障 消息代理的性能問題 等等

更麻煩的是 消息延遲還可能導致無法預測消息順序 比如說事件A比事件B先被發出 但事件B先到達流處理器 即使它們實際上是以相反的順序發生的

所以 將事件時間和處理時間搞混會導致錯誤的數據

#### 既然可能延遲 那我就用事件時間當基準吧

用事件時間來定義窗口 也有一個棘手的問題 那就是你永遠也無法確定是不是已經收到了特定窗口的所有事件 還是說還有一些事件還在來的路上

比如說你要分析每分鐘的請求數 10:37的數據已經接收處理得差不多了 但你要等到幾點幾分才能確定再也不會有10:37這個分鐘裡面的事件再傳過來呢

假設你等到10:40 你處理完資料後 有一個事件來了 你有兩個處理方式

1.忽略這些滯留(straggler)事件: 因為這通常只是一小部分 而且還可以監控滯留事件的數目 如果太多就發出警報

2.發佈一個更正(correction): 一個包括滯留事件的更新窗口值 比較麻煩 需要收回以前的分析輸出


#### 你用的是誰的時鐘


更麻煩的問題來了 如果一個事件可能在很多不同的地方進行緩衝(buffered) 要為這個事件發配時間戳就更難了


比如說 蘋果手機有些應用 會不定時向蘋果主機回報一些資訊 比如用量等等 但如果手機連不到網路 這些事件就會被待在手機裡緩衝 等到下次連到網路的時候再向服務器上報 這可能是幾小時或是幾天
對於這個流的任意消費者而言 它們就如延遲極大的滯留事件

在這個例子中 事件應該要被記錄成用戶交互發生的時間 取決時移動設備的本地時鐘 但用戶控制的設備上的時鐘通常是不可信的

要校正不正確的設備時鐘 一般來說需要記錄三個時間戳

1.事件發生的時間 取決於設備時鐘

2.事件發送往服務器的時間 取決於設備時鐘

3.事件被服務器接收的時間 取決於服務器時鐘

通過從第三個時間戳中減去第二個時間戳 可以估算設備時鐘和服務器時鐘之間的偏移(網絡延遲忽略) 然後可以將事件時間戳減去這個偏移 就有事件實際發生的真實時間

#### 窗口的類型

當你知道如何確定一個事件的時間戳後 下一步就是如何定義時間段的窗口 以下是常用的窗口

1.滾動窗口(Tumbling Window): 滾動窗口有著固定的長度 每個事件都僅能屬於一個窗口

假設你有一個1分鐘的滾動窗口 則所有時間戳在`10:03:00`和`10:03:59`之間的事件會被分組到一個窗口中 `10:04:00`和`10:04:59`之間的事件被分組到下一個窗口

2.跳動窗口(Hopping Window)

跳動窗口也有著固定的長度 但允許窗口重疊 

假設你有一個長度五分鐘 hopping 1分鐘的窗口 那第一個窗口包含`10:03:00`至`10:07:59`之間的事件 而下一個窗口將覆蓋`10:04:00`至`10:08:59`之間的事件 依此類推


3.滑動窗口(Sliding Window)

滑動窗口包含了彼此間距在特定時長內的所有事件 沒有一個很準確的分隔 無時無刻都在多動

比如一個5分鐘的滑動窗口 應該包含`10:03:39`和`10:08:12`這兩個事件 因為他們差距小於五分鐘 但跳動或滾動則會把他們分開 因為他們有很明確的邊界


4.會話窗口(Session window)

會話窗口沒有固定的持續時間 我們將一用戶出現時間相近的所有事件分組在一起 等到用戶很久沒動作了(比如30min) 我們就定義窗口結束 

以Session來區分事件是個很常見的方式

### 流式連接 Stream Joins

我們知道批次處理中很常會需要Join兩個不同的table 在流處理中也一樣 但是對於一個隨時會出現的事件 這困難度就比已經批處理高上太多 

為了更簡單瞭解 我們分成三種情況 stream-stream join, stream-table join, table-table join

#### stream-stream join(流流連接, 窗口連接)

用例子切入 你的搜尋網站有兩個stream: 

1.當使用者搜尋一個詞的時候 發出一個event 包含查詢的詞和返回結果

2.當使用者點擊其中一個返回結果 就發出另一個event 記錄這個點擊事件

那今天如果我想要計算網站中給定一個搜索詞時每一個URL的點擊率  就必須將這兩個流(**搜索**和**點擊**)利用sessionId join起來

你可能會說 為什麼不在點擊的時候 順便記錄搜索的詞就搞定了 幹嘛要JOIN?

如果我們這樣做 我們就無法知道使用者搜索後但都不點擊的情況 而這通常也是一個搜尋引擎想知道的情況(可能代表你的網站不夠好 使用者查完後對於返回結果沒興趣)

在這個例子 連接Join兩個流是必須的

要做到這點 流處理器需要維護一個**狀態**(state) 比如說在這個例子 我們需要按照sessionId去index最近一小時內發生的事件

當有搜尋事件或是點擊事件發生時 就會被加到正確的sessionId的索引上 如果同一個索引上在一小時內看到了這兩種事件 就再發出一個**搜尋且點擊**的事件 如果只有搜尋事件 就發出**搜尋且未點擊**的事件

#### stream-table join(流表連接, 流擴展)

上例子 我們有一個描述用戶活動事件的流(事件中包含userId) 還有一個數據庫 裡面有每個userId的更多資料比如userName, userAge等等

我們想要消費這個描述用戶活動事件的流 消費之後發一個數據庫查詢**擴充(enriching)**這個事件的資料 然後再發出另一個流

這是很常見的應用

我們可以將數據庫副本加載到流處理器中 以便在本地進行查詢而無需網路往返 如果副本夠小 甚至可以直接存成hashMap放在內存 大一點也可以存成index放在硬碟

那如果存在內存 使用者突然更新的檔案怎麼辦 比如說突然改名字或是改地址?

沒錯 我們可愛的[Change Capture](/2019/07/21/databases-and-streams/#變更數據捕獲
-change-data-capture)又發揮用途了 我們讓流處理器同時消費兩個事件 活動事件跟使用者數據變更事件 當發生活動事件時 把userId去查詢內存的本地資料庫擴充事件 當發生數據變更事件時 更新本地資料庫

流表連接實際上非常類似於流流連接 你就把表當成一個**可以追溯到最一開始的流**

#### table-table join(表表連接, 維護物化視圖)

其實就是Database的table join 

### 容錯

我們來到的流處理章節的最後一節 了解一下流處理如何容錯

我們知道批處理很容易容錯 任何一個MapReduce出錯的話 可以簡單地在另一台機器上再次啟動

#### 微批量與存檔點

#### 冪等性

#### 失敗後重建狀態
#### 原子提交再現





















####################
我們可以從消息傳遞和流中獲取靈感 應用在數據庫上 而事實上 [**複製日誌**](/2019/02/12/replication/#複製日誌的實現)就是數據庫寫入事件的流 由主庫在處理事務時生成 從庫將寫入流應用到它們自己的數據庫副本 從而最終得到相同數據的精確副本


在本節中 我們將首先看看數據系統中出現的一個問題 然後探討如何通過將事件流的想法帶入數據庫來解決這個問題


### 保持系統同步

本書講到這裡 大家應該知道沒有一種系統可以滿足所有的數據**存儲/查詢/處理**需求 在實踐中 大多數重要應用都需要組合使用幾種不同的技術來滿足所有的需求

比如用OLTP來為用戶請求提供服務 使用緩存來加速常見請求 使用全文索引搜索處理搜索查詢 使用數據倉庫用於分析 等等 每一個component都維持著自己的數據副本 各個系統再以自身的use case進行優化

但也因為相同的數據出現在了不同的地方 所以相互間需要保持同步 比如說一個數據在數據庫被更新了 那他也應該在緩存, 搜索索引, 數據倉庫中一起被更新 這種同步通常由[ETL](/2019/01/19/storage-and-retrieval/#數據倉庫-data-warehouse)處理

如果一個週期的完整數據ETL太慢 有時候替代方案會是Dual-Write: 由應用程式明確地寫入各個系統 比如說寫入數據庫 再更新搜索索引 再使緩存失效

但Dual-Write也有著嚴重的問題 

1.Race condition

在下圖的例子中 客戶端1想要將值設置為A  客戶端2想要將其設置為B 應用程式的邏輯是先改數據庫 再改搜索索引
![Alt text]({{ site.url }}/public/DDIA/DDIA-11-4.png)

但是運氣不好 所以數據庫最後的X值跟搜索索引最後的X值不一樣

2.其中一個寫入可能會失敗 而另一個成功

你要馬讓兩個都成功 要馬讓兩個都失敗 而要做到Atomic commit是非常昂貴的

### 怎麼辦
保持系統同步的最大難題是因為有多個領導者(數據庫 搜尋引擎 數據倉庫等等) 如果我們能讓其中一個變成唯一領導者 其他的變成追隨者 問題就搞定了

但有可能嗎?

### 變更數據捕獲 Change Data Capture

大多數數據庫的複製日誌的問題在於 它們一直被當做數據庫的內部實現細節 而不是公開的API 

客戶端應該通過其數據模型和查詢語言(比如SQL)來查詢數據庫 而不是解析複製日誌並嘗試從中提取數據

但如果我們能夠把一個數據庫的所有寫入數據變更 以流的形式發出 讓其他追隨者消費 那就天下太平

![Alt text]({{ site.url }}/public/DDIA/DDIA-11-5.png)

#### 變更數據捕獲的實現

我們可以將日誌消費者叫做衍生數據系統(比如存儲在搜索索引和數據倉庫中的數據) 只是記錄系統數據的額外視圖 

變更數據捕獲是一種機制可確保對記錄系統所做的所有更改都反映在衍生數據系統中

本質上來說 變更數據捕獲使得一個數據庫成為領導者 其他組件變成追隨者

比較直觀的實現是介由[數據庫觸發器](/2019/02/12/replication/#trigger-based的複製) 通過註冊觀察所有變更的觸發器 將相應的變更項寫入變更日誌表中 但性能開銷很大 解析複製日誌可能是一種更穩健的方法

LinkedIn的Databus, Facebook的Wormhole, Yahoo!的Sherpa都有著大量的變更數據捕獲的應用

像消息代理一樣 變更數據捕獲通常是異步的 也就是我發我的Change capture 消費者消費你的 我並不等你消費完我才進行提交 好處是緩慢的消費者不會影響到整個系統 壞處是所有[複製延遲可能有的問題](/2019/02/12/replication/#section-6)在這裡都可能出現

#### 初始快照

如果你擁有所有對數據庫進行變更的日誌 那你當然可以通過重放該日誌 來重建數據庫的完整狀態 但永遠保存所有更改會耗費太多磁盤空間 所以解法還是每隔一段時間要有一個系統的快照 如同[設置新從庫](/2019/02/12/replication/#設置新從庫)中所提到的解法

當然每個數據庫的快照 都需要有一個相對應的變更日誌的偏移量 這樣消費者才知道我重放最新的快照之後 要從哪裡開始消費變更數據捕獲

#### 日誌壓縮

有一個值得一提的節省變更日誌空間的方式 就是把所有對於相同key的更改中 舊的更改給拋棄 

比如說 數據庫曾經更改

X=1

Y=2

X=3

Z=4

X=5

Delete Y

那你可以壓縮你的日誌變成


Z=4

X=5

DeleteY

概念就是這麼簡單

所以現在當你想重建或新增一個衍生數據系統 你可以從壓縮日誌主題0偏移量處啟動新的消費者 然後一次掃過日誌中的所有消息 也可以達到同樣的要求

Apache Kafka支持這種日誌壓縮功能 這讓一個消息代理**可以被當成持久性存儲使用**

### 事件溯源 Event Sourcing

我們可以對數據庫下查詢來知道應用現在的狀態 這可以回答很多問題 但有時候我們不是只想知道where we are, 我們還想知道how we got there

事件溯源就是把所有對於數據庫的改變存儲成一系列的不可變的事件 不只我們可以知道現在的狀態 我們還可以知道過去任何一個時候的狀態

#### 事件日誌中推斷出當前狀態

事件日誌**本身**並不是很有用 因為用戶通常期望看到的是系統的當前狀態 而不是變更歷史 比如在購物車上 用戶只想看到目前購物車有什麼 而不是我過去曾放進來什麼拿出去什麼

所以 使用事件溯源的應用 需要把事件日誌轉換成應用程式的狀態 這個轉換必須是deterministic的 這樣你在系統出錯的時候可以再跑一次 達到一樣的狀態

當然也不是每次系統掛掉後 為了重建狀態就每次都從Day0重新爬log 每過一段時間還是可以拍一些快照 加快重建速度

#### 命令和事件

我們需要仔細的區分命令跟事件 當來自用戶的請求到來時 那是個命令 在這個時間點上它仍然可能可能失敗(可能違反了一些條件 比如想領超過你銀行帳戶的錢) 所以應用必須先驗證它是否可以執行該命令 如果驗證成功並且命令被接受 則它變為一個持久化且不可變的事件

在事件生成的時刻 它就成為了事實(fact) 即使客戶稍後決定更改或取消預訂 他們之前曾經預定了某個特定座位的事實仍然成立

事件流的消費者不允許拒絕事件 當消費者看到事件時 它已經成為日誌中不可變的一部分 任何對命令的驗證 都需要在它成為事件之前同步完成

### 狀態, 流和不變性

批處理因其輸入文件不變性而受益良多 你可以在現有輸入文件上運行實驗性處理作業而不用擔心損壞它們

這種不變性原則也是使得事件溯源與變更數據捕獲如此強大的原因

**數據庫 是應用程序當前狀態的存儲**

有當前狀態 就表示狀態會變化 所以數據庫支持數據的增加刪減修改 只要你的狀態發生修改 這個狀態就是這段時間中事件修改的結果 比如說目前的訂位狀況是一系列定位事件的結果 當前帳戶餘額是一系列提款匯款的結果

如果你傾向於數學表示 那應用狀態是事件流對時間求積分得到的結果 而變更流是狀態對時間求微分的結果

![Alt text]({{ site.url }}/public/DDIA/DDIA-11-6.png)

#### 不可變事件的優點

數據庫中的不變性是一個古老的概念 比如會計在幾個世紀以來一直在財務記賬中應用不變性 一筆交易發生時 它被記錄在一個append-only寫入的分類帳中

如果發生記帳錯誤 會計師不會刪除或更改分類帳中的錯誤交易 而是添加另一筆交易以補償錯誤 讓不正確的交易永遠留在分類帳中 對於之後的審計可能非常重要

同樣的概念也適用於批處理 如果你意外地部署了將錯誤數據寫入數據庫的錯誤代碼 導致代碼會破壞性的**修改**數據 恢復這個錯誤會非常麻煩 但如果你是使用不可變事件的append-only日誌 診斷問題與故障恢復就要容易的多

不可變的事件還有另外的優點 他除了當前狀態之外還有更多的訊息 比如說我把A放進購物車 再把A移除 再把B放進購物車 對於單純修改數據庫的實作而言 他們就只能照著顧客要求下正確訂單 但如果有記錄一系列不可變事件的話
你還可以利用這個消費者曾經考慮過A的事件 在未來給他推薦其他東西

#### 並發控制

事件溯源和變更數據捕獲的最大缺點是: 事件日誌的消費者通常是異步的

所以可能會發生的情況是: 用戶會寫入日誌 然後從日誌衍生視圖中讀取 結果發現他的寫入還沒有反映在讀取視圖中
 我們曾在[讀己之寫](/2019/02/12/replication/#讀己之寫)中提到類似的問題

一個解法是是將事件附加到日誌時 同步執行讀取視圖的更新 將這兩個寫入操作當作一個atomic trasaction

同樣的概念 從事件日誌導出當前狀態也簡化了並發控制的某些部分 許多對於多對象事務的需求是源自於**單個用戶操作**需要在不同地方改變數據(參考[多對象操作](/2019/04/21/transactions/#單對象和多對象操作)) 
透過事件溯源 你可以自己定義一個self-contained的事件 然後用戶操作就只需要在一個地方進行單次寫入操作 並將這個事件附加到日誌中 這樣就很好原子化

如果事件日誌與應用狀態以相同的方式分區(比如處理分區3中的客戶事件 只需要更新分區3中的應用狀態) 那麼直接使用單線程日誌 消費者就不需要寫入並發控制 日誌通過在分區中定義事件的序列順序 消除了並發性的不確定性


#### 不變性(Immutability)的限制

講了那麼多不變性的好處 來講講限制吧

永遠保留所有變更的不變歷史 到底有多可行呢 答案取決於數據集的流失率 你可以把它想成是更新率

如果一個數據庫的工作大部分都是添加數據 很少更新和刪除 那要維持不變是很簡單的 

但如果一個數據庫的工作大部分都是修改數據跟刪除數據 那要維持不可變的歷史記錄可能會很辛苦 碎片化處理不易 壓縮資料和垃圾收集的表現對於運維的穩健性變得至關重要

除了性能方面的原因之外 也可能有出於管理方面的原因需要刪除數據的情況 比如說大名鼎鼎的GDPR 這種情況你就不能只是append一個日誌說要刪除舊數據 你必須真的刪除它 並假裝這個數據從未存在過

而你真正想刪除數據是很難的 因為副本可能存在於很多地方 而且副本通常是做成不可變的 來防止意外刪除或是破壞
