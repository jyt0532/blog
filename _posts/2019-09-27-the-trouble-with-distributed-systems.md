---
layout: post
title: Designing Data-Intensive Application - The Trouble with Distributed Systems
comments: True 
subtitle: 分佈式系統的麻煩
tags: systemDesign 
author: jyt0532
excerpt: 
---

這是Designing Data-Intensive Application的第二部分第四章節: 分佈式系統的麻煩

本文所有圖片或代碼來自於原書內容
{% include copyright.html %}

# 前言

在前幾章中反覆出現的問題是 系統該如何處理錯誤情況 比如[副本故障切換](/2019/02/12/replication/#section-4) [複製延遲](/2019/02/12/replication/#複製延遲問題) 或是[事務控制](/2019/04/29/weak-isolation-levels/)

即使我們在之前談了很多錯誤 但我們還不夠悲觀 這篇文章我們要把悲觀最大化 假設任何可能出錯的東西都出錯

![Alt text]({{ site.url }}/public/pessimistic.jpeg)

我們會討論所有分布式系統**實踐上**可能會遇到的問題 了解哪些東西是我們能依賴的 哪些東西是我們不能過分依賴的

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
本章對分佈式系統中可能出現的問題進行徹底的悲觀和沮喪的總結 我們會討論

1.網絡的問題（“無法訪問的網絡”

2.時鐘和時序問題（“不可靠時鐘”）

3.我們將討論他們可以避免的程度。 所有這些問題的後果都是困惑的，

所以我們將探索如何思考一個分佈式系統的狀態，

以及如何推理髮生的事情（“知識，真相和謊言”）
@@@@@@@@@@@@@@@@@

## 故障與部分失效

單個計算機上的軟體沒有根本性的不可靠原因 當硬體正常工作時 相同的操作總是產生相同的結果 我們稱為deterministic 如果你有硬體上的問題 那你通常會是整個系統故障 所以一個跑軟體的個人計算機 要馬功能完好 要馬整組壞掉 不會介於兩者之間

但如果你是運行在多台計算機上的軟體時 那情況就完全不同 分布式系統並不是理想的模型很有可能系統的**某部分**會以不可預知的方式被破壞 我們稱為**部分失效(partial failure)** 頭痛的地方在於部分失效是**不確定性的(nonderterministic)**

如果你試圖想做涉及多個節點和網路的事情 這有時會成功有時會失敗 更慘的是如果是網路壞掉 你甚至無法知道你想做的事是否成功了 因為消息通過網路傳播的時間也是不確定的

這種不確定性和部分失效的可能性 使得分佈式系統難以工作


### 雲計算與超級計算機

關於如何建構一個大型計算機有一個光譜

1.光譜的一端 是使用HPC(High-performance computing) 就是一個擁有數千CPU的超級猛電腦 可以用來計算密集的計算機任務 比如說天氣預報或是分子動力學

2.光譜的另外一端 則是雲計算(cloud computing) 通常是多個數據中心以乙太網路連結 彈性的按照需求/流量計費

3.一般企業使用的數據中心模型就是介於兩者之間

在光譜的不同地方 會導致不同的故障處理方式 在超級計算機的那一端 作業通常會不時地會將計算的狀態存盤到持久存儲中 如果一個節點出現故障 通常的解決方案是簡單地停止整個集群的工作負載 故障節點修復後 計算從上一個檢查點重新開始

因此 超級計算機更像是一個單節點計算機而不是分佈式系統 它通過讓部分失敗升級為完全失敗來處理部分失敗

但在本書中 我們希望處理的是互聯網服務的系統 這些系統通常與超級計算機有很大不同

1.許多與互聯網有關的應用程序都是Online的 我們必須要一直available! 讓服務暫停是不可接受的 不像天氣預測的批處理計算 可以隨時停止之後重新啟動

2.超級計算機通常由專用硬件構建而成 每個節點相當可靠 雲服務的節點是由商品機器構建而成的 雖然因為規模經濟一次購買大量比較便宜 但具有較高的故障率

3.超級計算機通常使用專門的網絡拓撲結構 例如多維網格和環面 這些為HPC工作負載提供了更好的性能 大型數據中心網路則是基於IP和已太網 以閉合拓撲排列提供更高的二等分帶寬

4.系統越大 其組件之一就越有可能發生變化 當你修好一個壞掉的東西時 別的東西可能也壞了 在一個有成千上萬個節點的系統中 我們可以合理的認為總是有一些東西壞掉了 所以你必須一直分配部分的資源在修理節點上

5.如果系統可以容忍發生故障的節點 並繼續保持整體工作狀態 那麼這對於操作和維護非常有用 比如[滾動升級](/2019/01/24/encoding-and-evolution/)(一次重新啟動一個節點 系統繼續服務用戶不中斷)

6.在地理位置分布部署中(保持數據在地理位置上接近用戶以減少訪問延遲) 通信很可能通過互聯網進行 與本地網絡相比 通信速度緩慢且不可靠 超級計算機通常假設它們的所有節點都靠近在一起 


如果要使分佈式系統工作 就必須接受部分故障的可能性 並在軟件中建立容錯機制

換句話說 我們需要利用**不可靠的組件構建一個可靠的系統** 別忘了沒有完美的[可靠性](/2019/01/05/reliable-scalable-and-maintainable-application/) 我們需要理解我們可以實際承諾的限制

## 不可靠的網路

我們在本書中關注的都是[無共享的架構](/2019/09/27/distributed-data/#無共享架構) 網路就是機器跟機器間唯一的溝通途徑 意思就是說每台機器都不能訪問別台機器的硬碟跟內存

互聯網和數據中心中的大多數內部網路都是異步分組網路(asynchronous packet networks) 在這種網絡中 一個節點可以向另一個節點發送一個消息(一個數據包) 但是網路不能保證它什麼時候到達或者是否到達 可能出錯的地方如下

1.請求丟失(有人拔網路線)

2.請求正在queue裡排隊 等待晚點送達(網路超載或收件人超載)

3.遠程節點可能已經失效(崩潰或關機)
%%%%%%%%%%%%%%%%%%%%%%%%
4.遠程節點可能暫時停止了響應(垃圾回收暫停 參閱暫停進程)
%%%%%%%%%%%%%%%%%%%%%%%%%%
5.遠程節點可能已經處理了請求 但是網路上的響應已經丟失(網路交換機配置錯誤)

6.遠程節點可能已經處理了請求 響應已經被延遲 稍後才會送達(網路超載或寄件人超載)

當你發送一個封包且沒有得到回應 你並不知道是哪步出錯 可能(a)請求丟失 (b)遠端節點關閉 (c)響應丟失
 
![Alt text]({{ site.url }}/public/DDIA/DDIA-8-1.png)

處理這個問題的通常方法是**超時(Timeout)** 也就是過一段時間沒收到響應就放棄 但也不是放棄就沒事 你並不知道對方有沒有正確地收到這個消息並處理

### 檢測故障

許多系統需要自動檢測故障節點 比如說

1.負載平衡器需要停止向已死亡的節點發送請求 並把他移出輪值表(out of rotation)

2.單主複製功能的分佈式數據庫中 如果主庫失效 則需要將從庫之一升級為新主庫

不幸的是 因為網路的不確定性 使得你很難判斷一個節點是否工作 但在某些特定的網路故障我們有些明確的feedback

1.如果你可以到達運行節點的機器 但沒有進程正在listen to the port 這我們就可以從HTTP Status Code明確的知道是遠端問題不是網路問題

2.有可能在router那端它就知道你想要連接的ip位置已經死了 那就會給你一個特別的response

總的來說 如果出了什麼問題 你可能會在某個level得到有關錯誤的回應(router level, tcp level, application level) 但一般來說最常見的狀況 就是你根本沒有得到任何回應 你唯一能做的就是重試幾次 每次都timeout的話就基本可以確定對面節點掛了 

### 超時與無窮的延遲	

如果超時是檢測故障的唯一可靠方法 那麼超時應該等待多久? 不幸的是這題沒有簡單的答案

超時設定的太短 可能遠端節點只是處理得慢點你就說他掛了 把請求發給別人 到最後這個請求就會被做兩次

而且當你把請求轉移到其他節點時 會給其他節點和網路帶來額外的負擔 特別是那個你以為他死掉的節點 他會讓你覺得死掉很可能就是因為他現在手上的工作太多 你過早宣告他死亡只會讓事情更嚴重 因為他其實沒死掉 但你把他的工作交給別人 然後你就會以為別人死掉 然後就一連串的一個接一個真的死掉 我們稱為cascading failure

做個簡單的假設吧 

d: 網路可以保證一個數據被傳遞的最大延遲 每個數據包要碼在d內傳送 要麼丟失 但是傳遞永遠不會比d更長

r: 保證一個非故障節點總是在r時間內處理一個請求

在這種情況下 你就可以保證每個成功的請求在`2d + r`時間內都能收到響應 如果您在此時間內沒有收到響應 你就可以說網絡或遠程節點掛了

#### 網絡擁塞和排隊

計算機網路上數據包延遲的變數通常是由於排隊 而且有不少地方需要排隊

1.如果多個不同的節點同時嘗試將數據包發送到同一目的地 則網路交換機必須將它們排隊並將它們逐個送入目標

![Alt text]({{ site.url }}/public/DDIA/DDIA-8-2.png)

在請求很多的時候 數據包可能需要等待一段時間才能獲得一個插槽slot 如果傳入的數據太多 交換機隊列已經被填滿的話 數據包將被丟棄(需要重新發送數據包) 即使是在網路運作良好正常的情況下

2.而且當數據包到達目標機器時 如果所有CPU內核當前都處於繁忙狀態 則來自網路的傳入請求將被送到操作系統排隊 直到應用程序準備好處理它為止

3.在虛擬化環境中 正在運行的操作系統經常暫停幾十毫秒 由另一個虛擬機使用CPU內核 在這段時間內，虛擬機不能從網絡中消耗任何數據 所以傳入的數據也需要排隊

4.TCP執行流量控制(flow control) 也稱為擁塞避免(congestion avoidance)或背壓(backpressure) 其中發送節點限制自己的發送速率以避免網路或接收節點過載

> **TCP vs UDP**
>
> 一些對延遲敏感的應用程序(視訊 講電話等等) 使用UDP而不是TCP 這是在可靠性和和延遲可變性之間的折衷 由於UDP不執行流量控制並且不重傳丟失的分組 所以避免了可變網路延遲的一些原因

以上提的這些因素都會造成網路延遲的變化 通常你需要透過不斷的實驗 來測量延長的網路往返時間和多台機器的分佈 再考慮應用程序的特性 來決定**故障檢測延遲**與**過早超時風險**之間的適當折衷


### 延遲和資源利用

你可以把**延遲變化**視為**動態資源分區**的結果

假設兩台電話交換機之間有一條線路 可以同時進行10000個呼叫 通過此線路切換的每個電路都佔用其中一個呼叫插槽 
因此 您可以將線路視為可由多達10000個並發用戶共享的資源 **資源以靜態方式分配**: 意思就是即使您現在是電話上唯一的電話 並且其他9999個插槽都未使用 您的電路仍將分配與所有插槽都被使用時相同的固定數量的帶寬

相比之下 互聯網動態分享網路帶寬 發送者互相推擠已儘可能快地通過網路傳送 這種方法最大的好處就是無時無刻都最大化了資源的使用

CPU也會出現類似的情況 如果您在多個線程間動態共享每個CPU內核 則有一個線程有時必須等待操作系統的運行隊列 而另一個線程正在運行 這樣線程可以暫停不同的時間長度 但跟每個線程分配靜態數量的CPU週期相比 這會更好地利用硬件 而更好的硬件利用率也是使用虛擬機的重要動機

如果資源是靜態分區的(專用帶寬分配) 則在某些環境中可以實現**延遲保證**(就是我保證多久可以送的到)  但是 這是以降低利用率為代價的 換句話說就是比較昂貴

另一方面 動態資源分配的多租戶提供了更好的利用率 所以它更便宜 但它具有可變延遲的缺點

網路中的可變延遲不是一種自然規律 而只是成本/收益權衡的選擇

## 不可靠的時鐘(Clock)

應用程序以各種方式依賴於時鐘來回答以下問題

1.這個請求是否超時

2.這項服務的P99是多少

3.在過去五分鐘內 該服務平均每秒處理多少個查詢

4.用戶在我們的網站上花了多長時間

5.這篇文章在何時發佈

6.在什麼時間發送提醒郵件

7.這個緩存何時到期

8.日誌文件中此錯誤消息的時間戳是什麼

前四項測量的是持續的時間 後四項測量的是一個明確的時間點

在分佈式系統中 時間是一件棘手的事情 因為通信不是即時的 消息通過網路從一台機器傳送到另一台機器需要時間(收到消息的時間總是晚於發送的時間) 但網路上的每台機器都有自己的時鐘 這是一個實際的硬件設備(通常是石英晶體振盪器) 這些設備不是完全準確的 每台機器都還會有自己的時間概念 可能比其他機器稍快或更慢

我們可以利用網路時間協議(NTP)一定程度上同步時鐘 它允許根據一組服務器報告的時間來調整計算機時鐘

### 單調鐘(Monotonic clock)與時鐘(Time-of-day clock)

現代計算機至少有兩種不同的時鐘: 時鐘和單調鐘 儘管它們都衡量時間 但區分這兩者很重要

#### 時鐘Time-of-day clock

時鐘是你直觀地瞭解時間的依據: 它根據某個日曆(wall-clock time) 返回當前日期和時間 比如Linux的`clock_gettime(CLOCK_REALTIME)` 或Java的`System.currentTimeMillis()`

##### 單調鐘 Monotonic clock

單調鐘適用於測量持續時間(時間間隔) 比如超時或服務的響應時間

Linux的`clock_gettime(CLOCK_MONOTONIC)` 或Java的`System.nanoTime()`都是單調鐘 單調鐘的意思是 它們保證**總是前進**的事實(而Time-of-day clock有可能會被往回調)

你可以在某個時間點檢查單調鐘的值 做一些事情 且稍後再次檢查它 這兩個值之間的差異告訴你兩次檢查之間經過了多長時間

注意！單調鐘的絕對值是毫無意義的: 它可能是計算機啟動以來的秒數 或類似的任意值 特別是比較來自兩台不同計算機的單調鐘的值是沒有意義的 因為它們並不是一回事 比如開始計算的時間根本不一樣

如果NTP協議檢測到計算機的本地石英鍾比NTP服務器要更快或更慢 則可以調整單調鐘向前走的頻率(稱為slewing the clock) 預設情況是可以讓時鐘速率加快或減慢5%(但不能突然往前跳或往後跳)

在分佈式系統中 使用單調鐘測量經過**時間(elapsed time)**(比如超時)通常效果很好，因為它不假定不同節點的時鐘之間存在任何同步 並且對測量的輕微不準確性不敏感

#### 邏輯時鐘Logical Clock

Time-of-day clock和Monotonic clock也被稱為物理時鐘 還有一種是邏輯時鐘 它是基於遞增計數器而不是振盪石英晶體 對於排序事件來說是更安全的選擇 邏輯時鍾不測量一天中的時間或經過的秒數 而僅測量事件的相對順序(無論一個事件發生在另一個事件之前還是之後)

### 時鐘同步與準確性

Monotonic clock不需要同步 但Time-of-day clock就需要根據NTP服務器來對時 不幸的是 利用NTP對時沒有想像中的那麼簡單

$$$$$$$$$$$$$$EXAMPLES@@@@@@@@@@@@@@@@@@@


### 依賴同步時鐘

關於時鐘最大的問題 就是他們看起來很直觀 基本上不會出什麼錯 可是麻煩的地方在於 一天可能不會有精確的86400秒 時鐘可能會前後跳躍 更慘的是當時鐘出錯的時候很容易會被視而不見

如果是一個機器的CPU出錯或是網路出錯 可能根本就無法工作 所以很快就會有人來修 但如果是時鐘出錯 大部分事情都還可以正常運作 即使時鐘是很慢很慢的越差越多 那麼一個依賴於時鐘的軟件可能會就會無聲無息的一直做錯的事情 很久後才被注意到 而不是一次性的大崩潰


因此 如果你使用需要同步時鐘的軟件 **必須仔細監控所有機器之間的時鐘偏移** 時鐘偏離其他時鐘太遠的節點應當被宣告死亡 並從集群中移除

這樣的監控可以確保你在損失發生之前注意到破損的時鐘

#### 有序事件的時間戳

考慮一個很依賴時鐘的程序 - 依賴時鐘來對節點進行排序 比如說有兩個客戶端同時寫入分布式數據庫 誰先到達?

在這個栗子裡 Node3的時間慢了0.003秒
![Alt text]({{ site.url }}/public/DDIA/DDIA-8-1.png)

首先 客戶A先在Node1寫了`x=1` 寫入被複製到Node2跟Node3此時客戶B在節點3寫了`x++` 然後再被複製到Node1跟Node2 注意客戶B的寫入比較早到達Node2

對Node2而言 他發現兩個寫入 一個發生在42.004 一個發生在42.003 那這時候Node2怎麼辦呢 一個常見的解法是LWW(last writer win) 很暴力的捨棄客戶B的寫入

雖然LWW是很常見的解法 但還是有不少問題

1.一個時鐘落後的節點 他的寫入很常被忽視 直到這個時鐘被校準 在那之前很多寫入自然而然地被忽視也不會有人發現

2.LWW無法區分**真正高頻的寫入**跟**並發寫入** 需要額外的因果關係跟蹤機制 請參考[檢測並發寫入](/2019/02/12/replication/#檢測並發寫入)

3.兩個節點還是可能可以獨立生成具有相同時間戳的寫入 這時就需要一個隨機數來決定誰贏

儘管通過保留最**最近**的值並放棄其他值來解決衝突是很誘惑人的 但是要注意 **最近**的定義取決於本地的時鐘 這很可能是不正確的

#### 時鐘讀數存在信賴區間

您可能能夠以微秒或甚至納秒的分辨率讀取機器的時鐘 但將時鐘讀數視為一個時間點是沒有意義的 它更像是一段時間範圍 比如一個`clock_gettime()` 應該不是回傳一個`0.4` 應該回傳 `[0.3, 0.5]` 可惜的是大多數的系統不公開這種不確定性 跟你說多少就是多少

Spanner中的Google TrueTime API 明確的告訴了你本地時鐘的信賴區間 當你跟他問時間時 他會告訴你最早跟最晚 至於間隔多久取決於他上次跟準確的來源對時已經過了多久

#### 全局快照的同步時鐘

我們曾經討論過[快照隔離](/2019/04/29/weak-isolation-levels/#快照隔離-snapshot-isolation) 可以支持小型的快速讀寫事務 和大型的讀事務(用於備份或分析) 它允許只讀事務看到特定時間點的處於一致狀態的數據庫 且不會鎖定和干擾讀寫事務

快照隔離最常見的實現需要單調遞增的事務ID 如果寫入比快照晚 則該寫入對於快照事務是不可見的

在一個單節點數據庫上 一個簡單的計數器就足以生成事務ID 但是當數據庫分佈在許多機器上(也許可能在多個數據中心中時) 由於需要協調(跨所有分區) 全局單調遞增的事務ID可能很難生成 因為我們必須讓事務ID存在因果關係(比如事務B讀取由事務A寫入的值 則B必須具有比A更大的事務ID)

我們可以使用同步時鐘的時間戳作為事務ID嗎?如果我們能夠獲得足夠好的同步性 那麼這種方法將具有很合適的屬性: 更晚的事務會有更大的時間戳  當然還是老問題 我們無法保證時鐘精準度

看看有名的Spanner怎麼做吧 剛剛說他每問一個時間 都會回傳一個[min_time, max_time] 那如果現在有兩個事件如果事件A的max_time比事件B的min_time小 那天經地義A先發生 如果有重疊的話呢? 那Spanner就會等待到目前的事件A的max_time時間過了之後 再commit這個讀寫事務

### 暫停進程

讓我們考慮在分佈式系統中使用危險時鐘的另一個例子

假設你有一個數據庫 每個分區只有一個領導者 只有領導被允許接受寫入 一個節點如何知道它自己仍然是領導者(它並沒有被別人宣告為死亡) 並且它可以安全地接受寫入呢??

其中一種選擇是 這個領導者從其他節點獲得一個租約(lease) 類似一個帶timeout的鎖 
**任一時刻只有一個節點可以持有租約** 因此 當一個節點獲得一個租約時 它知道它在某段時間內自己是領導者 直到租約到期為止 而為了保持領導地位 領導節點必須週期性地在租約過期前續約

如果節點發生故障 就會停止續約 所以當租約過期時 另一個節點就可以安心接管

Code長得像這樣
{% highlight java %}
while(true){
	request=getIncomingRequest();
	// 確保租約還剩下至少10秒
	if (lease.expiryTimeMillis-System.currentTimeMillis()< 10000){
		lease = lease.renew();
	}

	if(lease.isValid()){
    	process(request);
	}}
}
{% endhighlight %}

這段程式有什麼問題? 首先 他的租約到期時間是由其他機器設置 但確認租約是否到期卻是依賴自己的同步時鐘

那如果我們把租約的時間用本地時間來設置呢 還有什麼問題嗎? 有的 因為這段程式假設

`System.currentTimeMillis()`到`process(request)`非常短 通常情況下 10秒的間隔已經可以保證租約不會在處理到一半過期

但今天如果在`lease.isValid()`之前停頓了15秒 然後跑到`lease.isValid()`的時候 別的機器事實上已經接管了leader的話 同樣的處理會被跑兩次

你說哪有可能停頓那麼久的 可惜在JAVA的世界裡面 很多情況都會讓進程停頓很久

1.垃圾收集器! 偶爾會需要停止所有正在運行的進程 特別是Stop the world GC

2.在虛擬化環境中 可以**suspend虛擬機**(暫停執行所有進程並將內存內容保存到磁盤)並**恢復**(恢復內存內容並繼續執行) 這個暫停可以在進程執行的任何時候發生 並且可以持續任意長的時間

3.你用的筆電也是 當你把筆電合起來 任何一個進程都可以被停止無數久 然後在你打開電腦的時候繼續

4.可以通過發送SIGSTOP信號來暫停Unix進程 例如通過在shell中按下Ctrl-Z

種種理由都可以暫時搶佔(preempt)正在進行的進程 並在稍後的時間恢復運行 而線程甚至不會注意到這一點

當我們在同一台機器編寫多線程代碼時 我們有很多工具可以讓程式線程安全 mutexes, semaphores, atomic counters, lock-free data structure, blocking queues等等 不幸的是 這些工具並不能直接轉化為分佈式系統操作 因為分佈式系統**沒有共享內存** 只有通過不可靠網路發送的消息

分佈式系統中的節點 必須假定其執行可能在任意時刻暫停相當長的時間 而且在暫停期間 世界的其它部分仍在繼續運轉



#### 響應時間保證Response time guarantee

某些軟件的運行環境要求很高 如果不能在特定時間內響應可能會導致嚴重的損失 比如說飛機操控台 或是無人車 機器人等等 這些機器必須要對輸入採取快速且可以預測的響應 

這些軟件必須有一個特定的截止時間(deadline) 如果響應時間太慢趕不上截止時間 可能會導致整個系統的故障 這就是所謂的**硬實時(hard real-time)**系統 

> 比如說車子的安全氣囊正在經歷碰撞 你一定不希望安全氣囊釋放系統因為GC暫停而延遲彈出

想要在一個系統中提供實時保證 需要各級軟件的支持 

1.實時操作系統(Real-Time OS): 允許在指定的時間間隔內保證CPU時間的分配

2.Library function必須記錄最壞情況下的執行時間 

3.Dynamic memory allocation可能受到限制或完全不允許

4.應用程式必須確保GC負擔不能太重

5.必須進行大量的測試和測量 以確保達到保證

所有的這些都需要大量的額外的工作 嚴重限制了可以使用的編程語言 函式庫和工具 畢竟大部分的工具不提供實時保證 值得一提的是 **實時和高性能**不一樣 事實上一個實時系統可能吞吐量低很多 因為他們必須優先考慮**及時響應**高於一切


對於大多數服務器端數據處理系統來說 實時保證是不經濟且不合適的 所以 這些系統必須承受在非實時環境中運行的暫停和時鍾不穩定性

#### 限制垃圾收集的影響

垃圾收集幾乎無法避免 但要怎麼樣讓垃圾收集的影響降到最低呢

第一個方法: 我們可以把GC當作一個節點的短暫暫停 當你快要需要GC的時候 先跟應用程序告知 讓他先不要傳請求給你 然後在你手上完全沒請求之後安心GC 回收完再繼續要請求

第二個方法: 只用垃圾收集器來處理短命對象 並定期在積累大量長壽對象之後 跑一個大個Full GC之前重啟進程 一次可以重新啟動一個節點 一樣在你打算重新啟動之前 流量可以從節點移開 就像滾動升級一樣

## 知識、真相與謊言







